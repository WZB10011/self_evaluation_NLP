# self_evaluation_NLP
学生信息
姓名：<魏振邦>

学号：<4231103>

课程：自然语言处理  
自我评价：90 / 100  
总体自我评价  
在本学期的《自然语言处理》课程中，我对自己的学习投入保持了相对客观、理性的态度。整体来看，我主要完成了课堂要求与作业内容，课余时间并未进行系统性的额外拓展学习。但在有限的投入下，我尽量保证对课程核心概念的理解，而不是仅停留在“完成任务”的层面。

本次自评将重点说明：  
1）我在课堂与作业要求范围内实际做了哪些事情；  
2）我在学习过程中真实获得了哪些理解；  
3）哪些方面投入不足，并据此给出合理的扣分说明。  
 评价依据  
第1–3周：课程进入与基础内容理解  
学习情况：  
按时参加课程，跟随课堂节奏学习 NLP 的整体框架，包括语言模型、分词、序列建模等基本概念。主要依赖课堂讲授与课件内容进行理解，没有额外系统阅读教材或论文。  
实际收获：
 对“语言建模”这一任务本身有了清晰认识，理解了 NLP 任务与传统机器学习在数据形式上的差异。  
反思：
此阶段主要是被动学习，对背景知识（如概率语言模型的历史演进）理解不够深入。  
第4–6周：作业驱动下的理解加深  
学习情况：  
 在完成相关作业的过程中，集中精力理解 RNN / LSTM 以及后续 Transformer 相关内容。
 学习方式以“为完成作业而查资料”为主，而非长期系统学习。  
 实际行为：

查阅过课程推荐的博客或讲义片段，以辅助理解作业中的概念（如序列建模、注意力机制的基本思想）。  
实际收获：虽然没有完整通读教材或论文，但通过作业反复接触，逐步建立了对模型结构的基本直觉，例如：注意力机制在序列建模中解决了什么问题； Transformer 与 RNN 在结构上的根本差异。  
第7–9周：Transformer 与语言模型相关内容  
学习情况：  
课堂内容进入本课程的核心部分（Transformer、Decoder-only 模型等）。
自身并未进行额外源码级深入阅读，主要依靠课堂讲解与作业要求理解模型结构。  
实际收获：
能够从整体结构层面理解 Transformer 的计算流程，包括embedding → attention → feed-forward → output 的主线逻辑；因果掩码在语言模型中的作用。  
不足之处：
对实现细节（如并行优化、训练工程技巧）了解较浅，仅停留在概念层面。  
 第10–13周：实验与作业阶段  
学习情况：
 按要求完成实验或编程作业，主要目标是跑通代码并理解输出结果的含义。
实验参数调试和结果分析较为基础，没有进行系统的对比实验。  
实际收获：
对语言模型训练的基本流程有了直观认识，包括数据输入形式；loss / perplexity 等指标的意义；简单生成结果与模型效果之间的关系。  
反思：实验更多是“验证课程内容”，而不是主动探索模型性能边界。  
第14–16周：复习与总结  
学习情况：
以复习课件、作业和课堂笔记为主，目标是巩固核心概念而非拓展新内容。  
实际收获： 能够在整体层面梳理 NLP 课程的知识结构，例如：任务层面（语言建模、文本生成）；模型层面（RNN → Transformer）；训练与评估的基本思路。  
不足：没有形成系统的个人学习笔记或博客总结。  


与他人及 AI 的交流情况  
与同学：在作业过程中与同学进行过零散讨论，主要集中在作业理解与实现思路上。  
与 AI：在理解部分概念或排查代码问题时，使用 AI 工具进行辅助解释，但更多是即时性使用，没有形成长期记录。  
编程与学习记录说明  
 编程活动主要集中在课程作业要求范围内；
未维护持续更新的 NLP 学习仓库或博客；
代码主要用于完成指定任务，而非独立项目探索。  
自我反思与总结我认为自己在这门课程中的定位更接近一名认真完成课程要求、但课外投入有限的学生。优点：能够在有限时间内理解并完成课程核心内容；对 NLP 的整体框架建立了清晰认识；作业完成态度认真，避免流于形式。  
不足：缺乏持续的课外深入学习；对前沿论文和工程细节了解较少；学习方式偏被动，探索性不足。  
自评分数说明（90 / 100）  
扣分依据：  
 1. 课余时间投入较少，未进行系统性的拓展阅读或实验探索（扣 5 分）； 2. 学习深度主要停留在课程要求层面，缺乏主动研究与总结（扣 5 分）。 因此，在“认真但投入有限”的前提下，我给自己的最终自评分数为 90 分。  
